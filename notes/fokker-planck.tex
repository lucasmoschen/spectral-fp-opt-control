\documentclass[12pt]{article}

\usepackage[english]{babel}
\usepackage[left=3cm, right=3cm, top=3cm, bottom=3cm]{geometry}
\usepackage{setspace}

\usepackage{amsmath, amssymb, amsfonts, amsthm}
\usepackage{cancel}
\usepackage{graphicx, float}
\usepackage{hyperref}
\usepackage{enumerate}

\title{Fokker-Plank equation}
\author{Lucas Moschen}
\date{\today}

% mathematical definitions 
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\n}{\vec{n}}
\newcommand{\steady}{\rho_{\infty}}
\newcommand{\inner}[2]{\langle{} #1, #2 \rangle{}}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{proposition}{Proposition}[subsection]
\newtheorem{remark}{Remark}[subsection]
\theoremstyle{definition}
\newtheorem{example}{Example}[subsection]
\newtheorem{definition}{Definition}[subsection]

% other
\newcommand{\sskip}{\vspace{5mm}}

\begin{document}

\maketitle

\onehalfspacing{}
    
\tableofcontents

\section*{Notation}

This list is part of the notation we use throughout the text:

\begin{itemize}
    \item $\langle X \rangle = \mathbb{E}[X]$ is the expected value of the random variable $X$.
    \item For $f, g \in H^1(\Omega)$, we define $\langle f, g \rangle = \int_{\Omega} f(x) g(x) \, dx$
    \item $\|f\|_{L^2}^2 = \int_{\Omega} \|f(x)\|^2 \, dx$.
\end{itemize}

\section{Introduction}

Consider a particle of mass $m$ immersed in a fluid.
The fluid applies a friction force on the particle which can be described by Stokes' law.
In this simple model, the velocity of the equation can be modelled through the following Ordinary Differential Equation (ODE)
\begin{equation}
    \label{eq:stokes-law}
    m \dot{v} + \alpha v = 0 \implies v(t) = v(0) \exp\left\{-\frac{\alpha}{m} t\right\} \overset{t \to \infty}{\to} 0.    
\end{equation}

However, for small values of $m$, thermal fluctuations may have a relevant effect on the particle's velocity.
To handle this case, we include a fluctuation force $F_f(t)$ of random nature in equation~\eqref{eq:stokes-law}, leading to 
\begin{equation}
    \label{eq:fluctuation-force}
    m\dot{v} + \alpha v = F_f(t).        
\end{equation}

The randomness simplifies the modelling of all particle interactions.
The {\em Langevin force\/} is given by the fluctuating force per unit of mass $\Gamma(t) = F_f(t)/m$.
In average, we expect that the velocity is determined by~\eqref{eq:stokes-law}. 
By this reason, we consider $\langle \Gamma(t) \rangle = 0$.
Moreover, $\langle \Gamma(t) \Gamma(t') \rangle = 0$ for $|t-t'| \ge \tau_0$, where $\tau_0$ is the duration time of a collision.

The Stochastic Differential Equation (SDE) in~\eqref{eq:fluctuation-force} induces a distribution on the velocity, since it is a random variable.
The probability density function (pdf) of the velocity for this model is given by a simple Fokker-Planck equation 
\[
\frac{\partial W}{\partial t} = \frac{\alpha}{m} \frac{\partial (v W)}{\partial v} + \frac{\alpha}{m} \frac{kT}{m} \frac{\partial^2 W}{\partial v^2},
\]
where $W(v,t)$ is the pdf of $v$ at time $t$.

The general form of Fokker-Planck equation in one dimension is given by 
\begin{equation}
    \label{eq:fokker-planck-1d}
    \frac{\partial W}{\partial t} = \left[-\sum_{i=1}^N \frac{\partial}{\partial x_i} D_i^{(1)}(x) + \sum_{i,j=1}^N \frac{\partial^2}{\partial x_i \partial x_j} D_{ij}^{(2)}(x)\right]W,
\end{equation}
where $D^{(1)}$ is the drift coefficient and $D^{(2)}$ is the diffusion coefficient.
Consider the SDE
\[
dX_t = -\nabla G(x) \, dt + \sqrt{2\nu} dB_t.
\]
The pdf $\rho$ of the process ${\{X_t\}}_{t \in \R_+}$ defined for $(x,t) \in \Omega \times (0,+\infty)$ is the solution to 
\[
\rho_t(x,t) = \nabla \cdot J(x,t),
\]
where $J(x,t) = \nu \nabla \rho(x,t) + \rho(x,t) \nabla G(x)$ is the probability current. 
This Partial Differential Equation (PDE) is subject to the boundary condition $J(x,t) \cdot \n = 0$ on $\partial \Omega \times (0,+\infty)$, which guarantees that for all $t \ge 0$,
\[
\int_{\Omega} \rho(x,t) \, dx = \int_{\Omega} \rho_0(x) \, dx,
\]
where $\rho_0(x) = \rho(x,0)$.
The steady state of this equation is the solution to
\[
\nabla \cdot J(x,t) = 0 \text{ in } \Omega \times \R_+,
\]
subject to $J(x,t) = 0 $ on $\partial \Omega \times \R_+$, when it does not depend on time.
Let $\rho(x,t) = c e^{-G(x)/\nu}$, where $c$ is a normalising constant given by the initial condition.
Therefore, 
\[
J(x,t) = \nu \left(-\frac{\nabla G(x)}{\nu} \rho(x,t)\right) + \rho(x,t) \nabla G(x) = 0,
\]
independent of $t$, which implies that the steady state $\steady$ is 
\[
\steady(x) \propto e^{-G(x)/\nu}.
\]

\section{Well-posedness}

We search for a solution $\rho \in W(0,T) = L^2(0,T; H^1(\Omega)) \cap H^1(0, t; {(H^1(\Omega))}^*)$ and 
\[
\inner{\rho_t}{\phi} + \inner{\nu\nabla \rho(t) + \rho(t)\nabla G}{\nabla \phi} + u(t)\inner{\rho(t)\nabla \alpha}{\nabla \phi} = 0, \forall \phi \in H^1(\Omega),
\]
where integration by parts was applied.
It is assumed that 
\[
G, \alpha \in W^{1,\infty}(\Omega) \cap W^{2,\max(2,n)}(\Omega).
\]
With that in mind, is possible to prove that for every $u \in L^2(0,T)$ and $\rho_0 \in L^2(\Omega)$, there exists an unique solution to Fokker-Planck equation.

Moreover the solution satisfies
\begin{enumerate}[(i)]
    \item For every $t \in [0,T]$, we have that $\inner{\rho(t) - \rho_0}{1_{\Omega}} = 0$.
    \item If $\rho_0 \ge 0$ almost everywhere on $\Omega$, then $\rho(x,t) \ge 0$ for all $t > 0$ and almost all $x \in \Omega$.
\end{enumerate}

Some additional references:

\begin{itemize}
    \item~\cite{bris2008existence}: covers the existence and the uniqueness of a class of Fokker-Planck equations with coefficients in Sobolov spaces $W^{1,p}$.
    \item~\cite{huang2015steady}: existence of steady state of Fokker-Planck equation in $\Omega \subseteq \R^n$.
\end{itemize}

\section{Optimal control problem}

To accelerate the convergence to the steady state as $t \to \infty$, we include a control function $u : \R_{+} \to \R$ which acts in the space according to the control shape function $\alpha : \Omega \to \R$, which satisfies $\nabla \alpha \cdot \n = 0$ on $\partial \Omega$.
Therefore, we substitute the potential $G$ to
\[
V(x,t) = G(x) + u(t) \alpha(x),
\]
Notice that with the format of $\alpha$, the boundary condition is unchangeable
\[
0 = (\nu \nabla \rho + \rho \nabla V) \cdot \n = (\nu \nabla \rho + \rho \nabla G) \cdot \n + \cancel{u~\rho (\nabla \alpha) \cdot \n}.
\]

Define $y = \rho - \steady$. 
We want that $\|y\|_{L^2}$ converges to $0$ the faster as possible. 
With the change of variables, the problem turns to
\[
\begin{split}
    y_t &= \nabla \cdot \left[\nu \nabla (y + \steady) + (y + \steady) \nabla V\right] \\
    &= \nabla \cdot \left(\nu \nabla y + y \nabla V + u\nabla \alpha \steady\right) + \cancel{\nabla \cdot (\nu \nabla \steady + \steady \nabla G)} \\
    &= \nabla \cdot (\nu \nabla y + y \nabla G) + u \nabla \cdot (y \nabla \alpha) + u \nabla \cdot (\steady \nabla \alpha),
\end{split}
\]
in $\Omega \times \R_+$ subject to 
\[
0 = (\nu \nabla (y + \steady) + (y + \steady)\nabla G) \cdot \n = (\nu \nabla y + y\nabla G) \cdot \n \text{ on } \partial \Omega \times \R_+
\]
and $y(x,0) = \rho_0(x) - \steady(x)$ in $\Omega$.
We can write the above problem as a bilinear abstract control system 
\[
\dot{y} = \mathcal{A} y + u\mathcal{N}y + \mathcal{B} u, \quad y(0) = y_0,
\]
where $\mathcal{B} = \mathcal{N} \steady$.

We look at the functional cost
\[
\mathcal{J}(y, u) = \frac{1}{2}\int_0^{\infty} \langle y, \mathcal{M} y \rangle + |u|^2 \, dt.
\]
Disregarding the bilinear expression $\N$, we get to the linear optimal control problem
\begin{align*}
    \min~ &J(y,u) \\
    \text{s.t. } &\dot{y} = \A y + \B u, \\
    &y(0) = y_0,
\end{align*}
where $\A$ incorporates the boundary condition $\nabla \cdot (\nu \nabla y + y \nabla G) = 0$.
The optimal control to that problem is obtained through solving the Ricatti equation
\[
\mathcal{A}^*\Pi + \Pi\mathcal{A} - \Pi\mathcal{B}\mathcal{B}^*\Pi + \mathcal{M} = 0
\]
with the feedback control $u = -\mathcal{B}^{*}\Pi y$.

The second strategy is based on the solution $\Gamma$ to a Lyapunov equation
\[
\mathcal{A}^* \Upsilon + \Upsilon \mathcal{A} + 2 \mu I = 0,   
\]
for a properly chosen parameter $\mu > 0$.

\section{Operator form}

Define the operators 
\[
\begin{split}
    \mathcal{A} &: \mathcal{D}(\mathcal{A}) \to L^2(\Omega), \\
    &\mathcal{D}(\mathcal{A}) = \{\rho \in H^2(\Omega) : (\nu \nabla \rho + \rho \nabla G) \cdot \n = 0 \text{ on } \partial \Omega\} \\
    &\mathcal{A} \rho = \nu \Delta \rho + \nabla \cdot (\rho \nabla G),   
\end{split}
\]
and
\[
\mathcal{N} : H^1(\Omega) \to L^2(\Omega), \mathcal{N}\rho = \nabla \cdot (\rho \nabla \alpha).    
\]
The adjoint operators are well-defined and given by 
\[
\mathcal{A}^*\varphi = \nu \Delta \varphi - \nabla G \cdot \nabla \varphi, (\nu \nabla \varphi) \cdot \n = 0 \text{ on } \partial \Omega    
\]
and 
\[
\mathcal{N}^* \varphi = - \nabla \varphi \cdot  \nabla \alpha. 
\]

Considering the uncontrolled system $\dot{\rho} = \mathcal{A} \rho$, introduce the function $\Phi(x) = \log \nu + \frac{G(x)}{\nu}$.
Further, define the operator $\mathcal{A}_s = e^{\Phi/2}\mathcal{A}e^{-\Phi/2}$.
Then,
\[
\mathcal{A}\left(e^{-\Phi/2} \rho \right) = \nu e^{-\Phi/2}\left(\Delta \rho + \frac{1}{2}\rho\Delta \Phi - \frac{1}{4}\rho \nabla \Phi \cdot \nabla \Phi \right).
\]
The following results can be obtained:
\begin{itemize}
    \item $\mathcal{A}_s$ is self-adjoint.
    \item The spectrum $\sigma(\mathcal{A}_s)$ consists of non-positive pure points with $0 \in \sigma(\mathcal{A}_s)$.
    \item The eigenfunctions ${\{\psi_i\}}_{i=0}^{\infty}$ form a complete orthogonal set.
    \item $\sigma(\mathcal{A}_s) = \sigma(\mathcal{A})$ and $\psi_i$ is eigenfunction of $\mathcal{A}$ iff $e^{\Psi/2}\psi_i$ is eigenfunction of $\mathcal{A}_s$. 
    \item $\psi_i$ is eigenfunction of $\mathcal{A}$ iff $e^{\Psi}\psi_i$ is eigenfunction of $\mathcal{A}^*$. 
    \item $\steady = ce^{-\Phi}$ is eigenfunction of $\mathcal{A}$ associated with the eigenvalue $0$.
\end{itemize}

\subsection{Decoupling the Fokker-Planck equation}

Consider the projection transform $\mathcal{P}$ onto $1^{\perp}$ along $\steady$, written as 
\[
\mathcal{P}y = y - \int_{\Omega} y \, dx \steady,    
\]
and its complementary $\mathcal{Q}y = (I-\mathcal{P})y$.
If $y \in \mathcal{Y}$, we can decompose $\mathcal{Y} = im(\mathcal{P}) \oplus im(\mathcal{Q})$, which implies $y = \mathcal{P}y + \mathcal{Q}y = y_{\mathcal{P}} + y_{\mathcal{Q}}$.
Therefore, Fokker-Planck equation turns to
\[
\dot{y}_{\P} + \dot{y}_{\Q} = \mathcal{A}(y_{\P} + y_{\Q}) + u\N(y_{\P} + y_{\Q}) + \B u.
\]
Applying $\P$ and $\Q$ to this equation and noticing that $\P(\P y) = \P y, \Q(\Q y) = \Q y, \P(\Q y) = \Q(\P y) = 0$, we get the system
\[
\begin{bmatrix}
    \dot{y}_{\P} \\ \dot{y}_{\Q} 
\end{bmatrix} = \begin{bmatrix}
    \P\A & 0 \\ 0 & 0 
\end{bmatrix}\begin{bmatrix}
    y_{\P} \\ y_{\Q} 
\end{bmatrix} + u\begin{bmatrix}
    \P\N & \P\N \\ 0 & 0 
\end{bmatrix}\begin{bmatrix}
    y_{\P} \\ y_{\Q} 
\end{bmatrix} + u\begin{bmatrix}
    \P\B \\ 0
\end{bmatrix},
\]
where other identities were also used.

\section{Ricatti-based feedback control}

It is considered the linearised version of the system, that is, $uNy$ term is dropped.
In this section, they derive the Ricatti equation.

\section{Methods for solving a PDE}

\begin{enumerate}[(a)]
    \item Finite differences:~\cite{chang1970practical} build a scheme where positivity and conservation of mass are maintained.
    \item Finite elements
    \item Collocation methods, which is a spectral method in the strong sense
    \item Spectral-Legendre method
\end{enumerate}

The equation we are trying to solve is 
\[
y_t = \nabla \cdot (\nu \nabla y + y \nabla G + u y \nabla \alpha + u \steady \nabla \alpha),
\]
subject to $(\nu \nabla y + y \nabla G) \cdot \n = 0$ in the boundary and $y = \rho_0 - \steady$ as initial condition.
In the weak formulation, for every $\phi \in H^1(\Omega)$, we have
\[
\inner{y_t}{\phi} = -\inner{\nu \nabla y + y \nabla G}{\nabla \phi} - u(t)\inner{\nabla \alpha y}{\nabla \phi} - u(t)\inner{ \nabla \alpha \steady}{\nabla \phi}.
\]

\subsection{Spectral-Legendre method}

Let us consider the one-dimensional case for now: $\Omega = [a,b]$.
To simplify future calculations, we consider the variable
\[
\tilde{y}(x,t) = y\left(\left(\frac{b-a}{2}\right)x + \left(\frac{a+b}{2}\right), t\right), \forall x \in [-1,1], t > 0.    
\]
We do the same for $G$, $\alpha$ and $\steady$. 
For sake of conciseness, we drop $\sim$ for now.
The formulation turns to 
\[
\inner{y_t}{\phi} = - {\left(\frac{2}{b-a}\right)}^2\inner{\nu y_x + y \dot{G} + u y \dot{\alpha} + u\steady \dot{\alpha}}{\dot{\phi}}.
\]

Consider the space 
\[
X_n = \{\phi \in P_n : \nu \dot{\phi}(\pm 1) + \phi(\pm 1) \dot{G}(\pm 1) = 0\},
\]
where $P_n$ is the space of polynomials with degree up to $n$.
Notice that $\dim(X_n) = n-1$.
Let ${\{\phi_i\}}_{i=0}^{n-2}$ be a basis for $X_n$ and write 
\[
y(x,t) \approx \sum_{j=0}^{n-2} y_j(t) \phi_j(x). 
\]
Considering the set of test equal to the trial functions, we get in the following formulation
\[
\begin{split}
    \sum_{j=0}^{n-2} \dot{y}_j(t) \inner{\phi_j}{\phi_i} = -{\left(\frac{2}{b-a}\right)}^2 &\sum_{j=0}^{n-2} y_j(t) \left(\nu \inner{\dot{\phi}_j}{\dot{\phi}_i} + \inner{\dot{G} \phi_j}{\dot{\phi}_i} + u(t)\inner{\dot{\alpha}\phi_j}{\dot{\phi}_i} \right) \\ 
    &+ u(t) \inner{\dot{\alpha} \steady}{\dot{\phi}_i},
\end{split}
\]
which can be rewritten as a system of ODEs:
\[
\Phi \dot{y}(t) = -{\left(\frac{2}{b-a}\right)}^2 (\Lambda + \Theta^1 + u(t)\Theta^2)y(t) - {\left(\frac{2}{b-a}\right)}^2u(t)v.    
\]
Following the suggestion from $\cite[p.7]{shen2011spectral}$, we consider
\[
\phi_k(x) = L_k(x) + \alpha_k L_{k+1}(x) + \beta_k L_{k+2}(x),    
\]
where $L_k$ is the Legendre polynomial of degree $k$ and the coefficients are chosen to satisfy the boundary conditions. 
Let us calculate these quantities:

\scalebox{0.85}{$
\nu (\dot{L}_k(\pm 1) + \alpha_k \dot{L}_{k+1}(\pm 1) + \beta_k \dot{L}_{k+2}(\pm 1)) + \dot{G}(\pm 1)(L_k(\pm 1) + \alpha_k L_{k+1}(\pm 1) + \beta_k L_{k+2}(\pm 1)) = 0.
$}

which can be written in matrix formulation
\[
\begin{bmatrix}
    \nu (k+1)(k+2) - 2g_{-} & -\nu (k+2)(k+3) + 2g_- \\
    \nu (k+1)(k+2) + 2g_+ & \nu (k+2)(k+3) + 2g_+
\end{bmatrix}\begin{bmatrix}
    \alpha_k \\ \beta_k
\end{bmatrix} = \begin{bmatrix}
    \nu k(k+1) - 2g_- \\ -\nu k(k+1) - 2g_+
\end{bmatrix},
\]
where $g_{\pm} = \dot{G}(\pm 1)$ and the system is numerically solved.

Let's now pre-calculate the matrices in terms of the legendre polynomials.

\[
\begin{split}
    \Phi_{ij} = \inner{\phi_i}{\phi_j} &= \inner{L_i}{L_j} + \alpha_i\inner{L_{i+1}}{L_j} + \beta_i\inner{L_{i+2}}{L_j} \\
    &+ \alpha_j\inner{L_i}{L_{j+1}} + \alpha_i\alpha_j\inner{L_{i+1}}{L_{j+1}} + \beta_i\alpha_j\inner{L_{i+2}}{L_{j+1}} \\
    &+ \beta_j\inner{L_i}{L_{j+2}} + \alpha_i\beta_j\inner{L_{i+1}}{L_{j+2}} + \beta_i\beta_j\inner{L_{i+2}}{L_{j+2}} \\
    &= 2\frac{\delta_{ij}}{2i + 1} + 2\alpha_i\frac{\delta_{i+1,j}}{2(i+1) + 1} + \beta_i\frac{\delta_{i+2,j}}{2(i+2) + 1} \\
    &= 2\alpha_j\frac{\delta_{i,j+1}}{2i + 1} + 2\alpha_i\alpha_j\frac{\delta_{ij}}{2(i+1) + 1} + 2\beta_i\alpha_j\frac{\delta_{i+1,j}}{2(i+2) + 1} \\
    &= 2\beta_j\frac{\delta_{i,j+2}}{2i + 1} + 2\alpha_i\beta_j\frac{\delta_{i,j+1}}{2(i+1) + 1} + 2\beta_i\beta_j\frac{\delta_{ij}}{2(i+2) + 1}
\end{split}
\]
\[
\begin{split}
    \Lambda_{ij} = \nu\inner{\dot\phi_i}{\dot\phi_j} &= \nu[\inner{\dot L_i}{\dot L_j} + \alpha_i\inner{\dot L_{i+1}}{\dot L_j} + \beta_i\inner{\dot L_{i+2}}{\dot L_j} \\
    &+ \alpha_j\inner{\dot L_i}{\dot L_{j+1}} + \alpha_i\alpha_j\inner{\dot L_{i+1}}{\dot L_{j+1}} + \beta_i\alpha_j\inner{\dot L_{i+2}}{\dot L_{j+1}} \\
    &+ \beta_j\inner{\dot L_i}{\dot L_{j+2}} + \alpha_i\beta_j\inner{\dot L_{i+1}}{\dot L_{j+2}} + \beta_i\beta_j\inner{\dot L_{i+2}}{\dot L_{j+2}}],
\end{split}
\]
where $\inner{\dot L_i}{\dot L_j} = L_{\max(i,j)}(1) \dot L_{\min(i,j)}(1) - L_{\max(i,j)}(-1) \dot L_{\min(i,j)}(-1)$, integrating by parts and observing that Legendre polynomials are orthogonal to other polynomials with smaller degree.
Therefore, 
\[
\inner{\dot L_i}{\dot L_j} = \frac{1}{2} \min(i,j) (\min(i,j) + 1)\left(1 + {(-1)}^{i+j}\right).
\]
Finally, we calculate
\[
\Theta^1_{ij} = \inner{\dot{G} \phi_j}{\dot \phi_i}, \Theta^2_{ij} = \inner{\dot{\alpha} \phi_j}{\dot \phi_i} \text{ and } v_i = \inner{\dot\alpha \steady}{\dot\phi_i}    
\]

This is the method of solving the PDE.\@
Before solving it, we need to compute the optimal control.
For that, we have to discretise the operators $\A, \B$ and $\mathcal{M}$:

The discretised versions are 
\[
A_{ij} = \inner{\A \phi_j}{\phi_i} = -{\left(\frac{2}{b-a}\right)}^2\inner{\nu \dot \phi_j + \dot G \phi_j}{\dot \phi_i} \implies A = -{\left(\frac{2}{b-a}\right)}^2(\Lambda + \Theta^1)
\]
and 
\[
B_{i} = \inner{\B\cdot 1}{\phi_i} = -{\left(\frac{2}{b-a}\right)}^2\inner{\steady \dot\alpha}{\dot\phi_i} \implies B = -{\left(\frac{2}{b-a}\right)}^2 v.
\]
With that in mind, we have to solve the discrete Ricatti equation
\[
A^T\Pi + \Pi A + \Pi B B^T \Pi + M = 0.
\]
with $u(t) = -B^T\Pi y(t)$. 
With this feedback, we solve
\[
\begin{split}
    \Phi \dot{y} &= (A + B^T\Pi y(t) \Theta^2 )y(t) - BB^T\Pi y(t) \\ 
    &= (A - BB^T\Pi  + B^T\Pi y(t) \Theta^2)y(t).
\end{split}
\]
After solving this system, we have to come back to the original coordinate system.

\begin{remark}
    Notice that $M_{ij} = \inner{\mathcal{M} \phi_j}{\phi_i}$.
    For instance, if $\mathcal{M}$ is the identity operator, we have $M = \Lambda$.
\end{remark}

\subsection{Finite elements}

The second method we consider is the finite elements method. 
In this case, we also consider the weak formulation and look for a basis ${\{\phi_i\}}_{i=0}^{n}$.
In the interval $[a,b]$, set the {\em node points\/} $a = x_0 < \cdots < x_n = b$.
The {\em elements\/} are the sub-intervals $[x_i, x_{i+1}]$ with $x_{i+1}-x_i=h$, for $i=0, \dots, n-1$.
We set the values $\phi_k(x_i) = \delta_{ik}$ and for each element a linear function. 
Therefore, define the function
\[
\phi(x) = \begin{cases}
    x/h, &\text{if } x \in[0,h] \\
    2 - x/h, &\text{if } x \in [h,2h] \\
    0, &\text{otherwise}
\end{cases} 
\]
and $\phi_{k}(x) = \phi(x-x_{k-1})$, for $k=0, \dots, n$ and $x \in [a,b]$, setting $x_{-1} = a-h$.
With that in mind, it remain to calculate the same matrices as the previous method.
Here, the rescale for $[-1,1]$ is not necessary.
Then, if $i \ge j$,

\[
\begin{split}
    \Phi_{ij} = \inner{\phi_i}{\phi_j} &= \int_a^b \phi(x-x_{i-1})\phi(x-x_{j-1}) \, dx \\
    &= \int_{0}^{2h} \phi(y)\phi(y+x_{i-1}-x_{j-1}) \, dy \\
    &= \int_{0}^{h} \frac{y}{h}\phi(y+(i-j)h) \, dy + \int_{h}^{2h} \left(2-\frac{y}{h}\right)\phi(y+(i-j)h) \, dy \\
\end{split}
\]
Notice that for $i-j\ge 2$, both integrals are $0$, since $\phi(y+2h) = 0$ for $y \ge 0$.
If $i=j+1$, the second integral is zero, while the first is 
\[
\int_0^h \frac{y}{h}\left(2-\frac{y+h}{h}\right) \, dy = h\int_0^1 z\left(1-z\right) \, dz = \frac{h}{6}.
\]
If $n > i=j > 0$, the integrals are 
\[
\int_{0}^{h} \frac{y^2}{h^2}\, dy = \frac{h}{3},  \quad \int_{h}^{2h} {\left(2-\frac{y}{h}\right)}^2 \, dy =  h\int_{0}^{1} z^2 \, dz = \frac{h}{3},
\]
which implies $\Phi_{ii} = 2h/3$ and, finally, $\Phi_{00} = \Phi_{nn} = h/3$. 
For $i < j$, $\Phi_{ij} = \Phi_{ji}$.
For calculating $\Lambda$, for $i \ge j$,

\[
\begin{split}
    \Lambda_{ij} = \nu \inner{\dot{\phi}_i}{\dot{\phi}_j} &= \int_a^b \dot\phi(x-x_{i-1})\dot\phi(x-x_{j-1}) \, dx \\
    &= \int_0^{2h} \dot\phi(y)\dot\phi(y + x_{i-1}-x_{j-1}) \, dx \\
    &= \frac{1}{h} \int_{0}^{h} \dot\phi(y+(i-j)h) \, dy - \frac{1}{h}\int_{h}^{2h} \dot\phi(y+(i-j)h) \, dy.
\end{split}
\]
If $i-j \ge 2$, both integrals are $0$.
If $i=j+1$, the second integral vanishes, while the first is 
\[
\int_0^h -\frac{1}{h} \, dx = -1 \implies \Lambda_{ij} = -\frac{\nu}{h}.
\]
If $i=j$, the first integral is $1$, while the second is $-1$, implying that $\Lambda_{ii} = 2\nu/h$.
Finally, if $i < j$, then $\Lambda_{ij} = \Lambda_{ji}$.

The other three matrices have some simplification, but need numerical integration.

\[
\begin{split}
    \Theta^1_{i,i+1} = \inner{\dot{G} \phi_{i+1}}{\dot\phi_{i}} &= \int_h^{2h} \dot{G}(y+x_{i-1})\phi(y-h)\dot\phi(y) \, dy \\
    &= -\frac{1}{h}\int_h^{2h} \dot{G}(y + (a+(i-1)h))\left(\frac{y-h}{h}\right) \, dy \\
    &= -\frac{1}{h}\left[\int_h^{2h} \dot{G}(y + a+ih-h)\frac{y}{h} \, dy - G(a + ih+h) + G(a + ih)  \right] \\
    &= -\frac{1}{h}\left[G(a + (i+1)h) - \frac{1}{h}\int_h^{2h} G(y+a+ih-h) \, dy \right] \\
    &= -\frac{1}{h}\left[G(a + (i+1)h) - \frac{1}{h}\int_{a+ih}^{a+(i+1)h} G(y) \, dy \right], \\
\end{split}    
\]
for $i=0,\dots,n-1$.
\[
\begin{split}
    \Theta^1_{i,i-1} = \inner{\dot{G} \phi_{i-1}}{\dot\phi_{i}} &= \int_0^{h} \dot{G}(y+x_{i-1})\phi(y+h)\dot\phi(y) \, dy \\
    &= \frac{1}{h}\int_0^{h} \dot{G}(y + (a+(i-1)h))\left(2-\frac{y+h}{h}\right) \, dy \\
    &= -\frac{1}{h}\left[\int_0^{h} \dot{G}(y + a+ih-h)\frac{y}{h} \, dy - G(a + ih) + G(a + (i-1)h)  \right] \\
    &= -\frac{1}{h}\left[G(a + (i-1)h) - \frac{1}{h}\int_0^{h} G(y+a+ih-h) \, dy \right] \\
    &= -\frac{1}{h}\left[G(a + (i-1)h) - \frac{1}{h}\int_{a+(i-1)h}^{a+ih} G(y) \, dy \right], \\
\end{split}    
\]
for $i=1,\dots,n$
\[
\begin{split}
    \Theta^1_{i,i} = \inner{\dot{G} \phi_{i}}{\dot\phi_{i}} &= \int_0^{2h} \dot{G}(y+x_{i-1})\phi(y)\dot\phi(y) \, dy \\
    &= \frac{1}{h} \left[\int_0^h \dot{G}(y+x_{i-1})\left(\frac{y}{h}\right) \, dy- \int_h^{2h} \dot{G}(y+x_{i-1})\left(2-\frac{y}{h}\right) \, dy \right] \\
    &= \frac{1}{h} \left[\int_0^{2h} \dot{G}(y+x_{i-1})\left(\frac{y}{h}\right) \, dy - 2(G(x_{i+1}) - G(x_i)) \right] \\
    &= \frac{1}{h} \left[2G(a+ih) - \frac{1}{h}\int_{a+(i-1)h}^{a+(i+1)h} G(y) \, dy \right], \\
\end{split}
\]
for $i=1,\dots,n-1$. 
For $i=0$, we only have the decreasing part.
\[
\Theta^1_{00} = -\frac{1}{h} \int_h^{2h} \dot{G}(y+(a-h))\left(2-\frac{y}{h}\right) \, dy = \frac{1}{h}\left[G(a)-\frac{1}{h}\int_a^{a+h} G(y) \, dy\right].
\]
On the other hand, for $i = n$, we only have the increasing part.
\[
\Theta^1_{nn} = \frac{1}{h} \int_0^{h} \dot{G}(y+(b-h))\left(\frac{y}{h}\right) \, dy = \frac{1}{h}\left[G(b) - \frac{1}{h}\int_{b-h}^b G(y) \, dy\right].
\]
On the other hand, for $i = n$, we only have the increasing part.
If $i - j \ge 2$, we have $\Theta_{ij}^1 = 0$.
Notice that $\Theta^2$ has similar calculations substituting $G$ by $\alpha$.

Finally, we calculate the vector $v$.
For $i=1,\dots,n-1$,
\[
v_i = \inner{\dot\alpha \steady}{\dot{\phi}_i} = \frac{1}{h}\left[\int_{a+(i-1)h}^{a+ih} \dot\alpha(y) \steady(y)  \, dy- \int_{a+ih}^{a+(i+1)h} \dot\alpha(y) \steady(y) \, dy \right]    
\]
and 
\[
v_0 = -\frac{1}{h} \int_a^{a+h} \dot\alpha(y) \steady(y)  \, dy, v_n = \frac{1}{h} \int_{b-h}^{b} \dot\alpha(y) \steady(y)  \, dy.
\]
The discrete matrices $A, B$ can be obtained from the same manner. 
Besides that, we need to consider the boundary condition: $\nu y_x + y G_x = 0$ in the boundary points.
Therefore, 
\[
\sum_{j=0}^{n} y_j(t) \left[\nu\dot\phi_j(x) + \dot{G}(x) \phi_j(x)\right] = 0, x=a,b, t \ge 0, 
\]
which defines to linear conditions, 
\[
y_0(t) \left[-\nu/h + \dot{G}(a)\right] + y_1(t) \left[\nu/h\right] = 0, y_{n-1}(t) \left[-\nu/h\right] + y_n(t) \left[\nu/h + \dot{G}(b)\right] = 0.
\]
So we can write $y_0(t) = \nu y_1(t)/(\nu-h\dot{G}(a))$ and $y_n(t) = \nu y_{n-1}(t) / (\nu + h\dot{G}(b))$.
This tells us that we can decrease the system of EDOs in two variables. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                 Bibliografia                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{biblio.bib}

\end{document}